@article{gelman1996PosteriorPredictiveAssessment,
  title = {Posterior {{Predictive Assessment}} of {{Model Fitness Via Realized Discrepancies}}},
  author = {Gelman, Andrew and Meng, Xiao-Li and Stern, Hal},
  date = {1996},
  journaltitle = {Statistica Sinica},
  volume = {6},
  number = {4},
  eprint = {24306036},
  eprinttype = {jstor},
  pages = {733--760},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  url = {https://www.jstor.org/stable/24306036},
  urldate = {2022-12-20},
  abstract = {This paper considers Bayesian counterparts of the classical tests for goodness of fit and their use in judging the fit of a single Bayesian model to the observed data. We focus on posterior predictive assessment, in a framework that also includes conditioning on auxiliary statistics. The Bayesian formulation facilitates the construction and calculation of a meaningful reference distribution not only for any (classical) statistic, but also for any parameter-dependent "statistic" or discrepancy. The latter allows us to propose the realized discrepancy assessment of model fitness, which directly measures the true discrepancy between data and the posited model, for any aspect of the model which we want to explore. The computation required for the realized discrepancy assessment is a straightforward byproduct of the posterior simulation used for the original Bayesian analysis. We illustrate with three applied examples. The first example, which serves mainly to motivate the work, illustrates the difficulty of classical tests in assessing the fitness of a Poisson model to a positron emission tomography image that is constrained to be nonnegative. The second and third examples illustrate the details of the posterior predictive approach in two problems: estimation in a model with inequality constraints on the parameters, and estimation in a mixture model. In all three examples, standard test statistics (either a χ2 or a likelihood ratio) are not pivotal: the difficulty is not just how to compute the reference distribution for the test, but that in the classical framework no such distribution exists, independent of the unknown model parameters.},
  keywords = {No DOI found},
  file = {/home/dm0737pe/MEGA/ZoteroLibrary/gelman_1996_posterior_predictive_assessment_of_model_fitness_via_realized_discrepancies.pdf}
}

@article{gneiting2007StrictlyProperScoring,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  date = {2007-03-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214506000001437},
  url = {https://doi.org/10.1198/016214506000001437},
  urldate = {2023-09-17},
  abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
  keywords = {Bayes factor,Bregman divergence,Brier score,Coherent,Continuous ranked probability score,Cross-validation,Entropy,Kernel score,Loss function,Minimum contrast estimation,Negative definite function,Prediction interval,Predictive distribution,Quantile forecast,Scoring rule,Skill score,Strictly proper,Utility function},
  file = {/home/dm0737pe/MEGA/ZoteroLibrary/gneiting_raftery_2007_strictly_proper_scoring_rules,_prediction,_and_estimation.pdf}
}

@incollection{held2010PosteriorCrossvalidatoryPredictive,
  title = {Posterior and {{Cross-validatory Predictive Checks}}: {{A Comparison}} of {{MCMC}} and {{INLA}}},
  shorttitle = {Posterior and {{Cross-validatory Predictive Checks}}},
  booktitle = {Held, {{L}}; {{Schrödle}}, {{B}}; {{Rue}}, {{H}}  (2010). {{Posterior}} and {{Cross-validatory Predictive Checks}}: {{A Comparison}} of {{MCMC}} and {{INLA}}. {{In}}: {{Kneib}}, {{T}}; {{Tutz}}, {{G}}. {{Statistical Modelling}} and {{Regression Structures}} - {{Festschrift}} in {{Honour}} of {{Ludwig Fahrmeir}}. {{Berlin}}: {{Physica-Verlag}} ({{Springer}}), 91-110.},
  author = {Held, L. and Schrödle, B. and Rue, H.},
  editor = {Kneib, T. and Tutz, G.},
  date = {2010},
  pages = {91--110},
  publisher = {{Physica-Verlag (Springer)}},
  location = {{Berlin}},
  doi = {10.1007/978-3-7908-2413-1},
  url = {https://www.zora.uzh.ch/id/eprint/36472/},
  urldate = {2023-09-17},
  abstract = {Model criticism and comparison of Bayesian hierarchical models is often based on posterior or leave-one-out cross-validatory predictive checks. Crossvalidatory checks are usually preferred because posterior predictive checks are difficult to assess and tend to be too conservative. However, techniques for statistical inference in such models often try to avoid full (manual) leave-one-out crossvalidation, since it is very time-consuming. In this paper we will compare two approaches for estimating Bayesian hierarchical models: Markov chain Monte Carlo (MCMC) and integrated nested Laplace approximations (INLA). We review how both approaches allow for the computation of leave-one-out cross-validatory checks without re-running the model for each observation in turn.We then empirically compare the two approaches in an extensive case study analysing the spatial distribution of bovine viral diarrhoe (BVD) among cows in Switzerland.},
  isbn = {978-3-7908-2412-4},
  language = {eng},
  file = {/home/dm0737pe/MEGA/ZoteroLibrary/held_2010_posterior_and_cross-validatory_predictive_checks_-_a_comparison_of_mcmc_and_inla.pdf}
}

@article{marshall2003ApproximateCrossvalidatoryPredictive,
  title = {Approximate Cross-Validatory Predictive Checks in Disease Mapping Models},
  author = {Marshall, E. C. and Spiegelhalter, D. J.},
  date = {2003-05-30},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  volume = {22},
  number = {10},
  eprint = {12720302},
  eprinttype = {pmid},
  pages = {1649--1660},
  issn = {0277-6715},
  doi = {10.1002/sim.1403},
  abstract = {When fitting complex hierarchical disease mapping models, it can be important to identify regions that diverge from the assumed model. Since full leave-one-out cross-validatory assessment is extremely time-consuming when using Markov chain Monte Carlo (MCMC) estimation methods, Stern and Cressie consider an importance sampling approximation. We show that this can be improved upon through replication of both random effects and data. Our approach is simple to apply, entirely generic, and may aid the criticism of any Bayesian hierarchical model.},
  language = {eng},
  keywords = {Bayes Theorem,Humans,Lip Neoplasms,{Models, Statistical},Predictive Value of Tests,Risk Assessment,Scotland,Small-Area Analysis},
  file = {/home/dm0737pe/MEGA/ZoteroLibrary/marshall_spiegelhalter_2003_approximate_cross-validatory_predictive_checks_in_disease_mapping_models.pdf}
}

@article{pettit1990ConditionalPredictiveOrdinate,
  title = {The {{Conditional Predictive Ordinate}} for the {{Normal Distribution}}},
  author = {Pettit, L. I.},
  date = {1990},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {52},
  number = {1},
  pages = {175--184},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1990.tb01780.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1990.tb01780.x},
  urldate = {2023-09-17},
  abstract = {The conditional predictive ordinate (CPO) is a Bayesian diagnostic which detects surprising observations. It has been used in a variety of situations such as univariate samples, the multivariate normal distribution and regression models. Results are presented about the most surprising observation which has minimum CPO. For the multivariate normal distribution it is shown that the most surprising observation must lie at one of the vertices of the convex hull. It is also shown that the observation with maximum Mahalanobis distance from the sample mean must lie on the convex hull. Results are given for the expected number of vertices on the convex hull when the sample is contaminated. An alternative, closely related diagnostic, the ratio ordinate measure, is presented. A numerical comparison of the two measures is given.},
  language = {en},
  keywords = {convex hull,mahalanobis distance,outliers,ratio ordinate,regression diagnostic},
  file = {/home/dm0737pe/MEGA/ZoteroLibrary/pettit_1990_the_conditional_predictive_ordinate_for_the_normal_distribution.pdf;/home/dm0737pe/Zotero/storage/4GKA7CKV/j.2517-6161.1990.tb01780.html}
}
